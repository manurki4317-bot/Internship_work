{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Electrophysiology Analysis Pipeline\n",
    "\n",
    "This notebook processes ABF files to detect spikes and bursts, classify bursts, extract normalized bursts for shapelet learning, train a shapelet model, perform UMAP embedding, and detect conflict regions between burst types.\n",
    "\n",
    "All sections include comments outside the code explaining their purpose."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 1. Imports and parameters\n",
    "# ================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyabf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks, hilbert\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.interpolate import interp1d\n",
    "from numpy.linalg import lstsq\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.shapelets import LearningShapelets\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import cm\n",
    "\n",
    "# Parameters\n",
    "folder_path = \"bursting\"  # Folder containing ABF files\n",
    "threshold = -35             # Spike detection threshold (mV)\n",
    "burst_threshold = 0.3       # Maximum ISI for bursts (s)\n",
    "fs = 10000                  # Sampling frequency (Hz)\n",
    "dt = 1/fs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 2. Helper functions for spikes and bursts\n",
    "# ================================\n",
    "def normalize_y(signal_segment):\n",
    "    return (signal_segment - np.mean(signal_segment)) / np.std(signal_segment)\n",
    "\n",
    "def rescale_x(time_segment, signal_segment, n_points=100):\n",
    "    f = interp1d(np.linspace(0, 1, len(signal_segment)), signal_segment)\n",
    "    return f(np.linspace(0, 1, n_points))\n",
    "\n",
    "def detect_spikes(signal, threshold=-35):\n",
    "    spike_indices, _ = find_peaks(signal, height=threshold)\n",
    "    return spike_indices\n",
    "\n",
    "def detect_bursts(spike_times, burst_threshold=0.3):\n",
    "    isi = np.diff(spike_times)\n",
    "    bursts = []\n",
    "    current_burst = [spike_times[0]]\n",
    "    for i in range(1, len(isi)):\n",
    "        if isi[i-1] < burst_threshold:\n",
    "            current_burst.append(spike_times[i])\n",
    "        else:\n",
    "            if len(current_burst) > 1:\n",
    "                bursts.append((current_burst[0], current_burst[-1]))\n",
    "            current_burst = [spike_times[i]]\n",
    "    if len(current_burst) > 1:\n",
    "        bursts.append((current_burst[0], current_burst[-1]))\n",
    "    return bursts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 3. Burst classification\n",
    "# ================================\n",
    "def classify_bursts(bursts, signal, time):\n",
    "    square_wave_bursts = []\n",
    "    parabolic_bursts = []\n",
    "    other_bursts = []\n",
    "\n",
    "    for i, (burst_start, burst_end) in enumerate(bursts):\n",
    "        burst_mask = (time >= burst_start) & (time <= burst_end)\n",
    "        burst_min = np.min(signal[burst_mask])\n",
    "\n",
    "        prev_mean = np.mean(signal[(time > bursts[i-1][1]) & (time < burst_start)]) if i>0 else np.nan\n",
    "        next_mean = np.mean(signal[(time > burst_end) & (time < bursts[i+1][0])]) if i < len(bursts)-1 else np.nan\n",
    "        inter_mean = np.nanmean([prev_mean, next_mean])\n",
    "\n",
    "        if burst_min > inter_mean:\n",
    "            square_wave_bursts.append((burst_start, burst_end))\n",
    "        elif burst_min < inter_mean:\n",
    "            parabolic_bursts.append((burst_start, burst_end))\n",
    "        else:\n",
    "            other_bursts.append((burst_start, burst_end))\n",
    "    return square_wave_bursts, parabolic_bursts, other_bursts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 4. Extract normalized bursts for shapelet learning\n",
    "# ================================\n",
    "def extract_normalized_bursts(burst_list, signal, time, n_points=100):\n",
    "    normalized_bursts = []\n",
    "    for start, end in burst_list:\n",
    "        mask = (time >= start) & (time <= end)\n",
    "        s_rescaled = rescale_x(time[mask], signal[mask], n_points)\n",
    "        s_normalized = normalize_y(s_rescaled)\n",
    "        normalized_bursts.append(s_normalized)\n",
    "    return normalized_bursts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 5. Shapelet model training (example placeholders)\n",
    "# ================================\n",
    "# After extracting normalized bursts, X and y can be created as follows:\n",
    "# X = np.array(all_normalized_bursts)[:, :, np.newaxis]\n",
    "# y = np.array(labels)\n",
    "# X = TimeSeriesScalerMinMax().fit_transform(X)\n",
    "# shp_clf = LearningShapelets(n_shapelets_per_size={X.shape[1]: 2}, weight_regularizer=0.0001,\n",
    "# optimizer=Adam(0.01), max_iter=300, verbose=0, scale=False, random_state=42)\n",
    "# shp_clf.fit(X, y)\n",
    "# distances = shp_clf.transform(X).reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 6. Segment metrics computation (example loop)\n",
    "# ================================\n",
    "all_segment_metrics = []\n",
    "abf_files = [f for f in os.listdir(folder_path) if f.endswith('.abf')]\n",
    "for file_name in abf_files:\n",
    "    abf = pyabf.ABF(os.path.join(folder_path, file_name))\n",
    "    for sweep in range(abf.sweepCount):\n",
    "        abf.setSweep(sweep)\n",
    "        signal = abf.sweepY\n",
    "        time = abf.sweepX\n",
    "        spike_indices = detect_spikes(signal)\n",
    "        spike_times = time[spike_indices]\n",
    "        bursts = detect_bursts(spike_times)\n",
    "        sq, par, oth = classify_bursts(bursts, signal, time)\n",
    "        # Compute metrics and append to all_segment_metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 7. UMAP embedding and visualization (example)\n",
    "# ================================\n",
    "# df_segments = pd.DataFrame(all_segment_metrics, columns=[...])\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(df_segments[feature_columns])\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "# embedding = reducer.fit_transform(X_scaled)\n",
    "# df_segments['UMAP1'] = embedding[:,0]\n",
    "# df_segments['UMAP2'] = embedding[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# 8. Conflict detection and plotting (example)\n",
    "# ================================\n",
    "# Identify conflict regions using nearest neighbors or UMAP binning\n",
    "# Plot conflicts and overlay traces, save to CSV if needed"
   ]
  }
 ]
}
