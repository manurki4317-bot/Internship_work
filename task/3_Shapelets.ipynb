{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike and Burst Detection, Normalization, and Shapelet Learning\n",
    "\n",
    "This notebook demonstrates a full pipeline for processing ABF files: spike detection, burst detection, burst normalization, classification, and shapelet learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Imports ====\n",
    "import pyabf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ABF file and concatenate sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"bursting/cell89basal.abf\"\n",
    "abf = pyabf.ABF(file_path)\n",
    "\n",
    "signal = np.concatenate([abf.setSweep(i) or abf.sweepY for i in range(abf.sweepCount)])\n",
    "dt = 1.0 / abf.dataRate\n",
    "time = np.arange(len(signal)) * dt\n",
    "print(f\"Loaded {file_path} | total duration: {time[-1]:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -35  # mV\n",
    "spike_indices, _ = find_peaks(signal, height=threshold)\n",
    "spike_times = time[spike_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_threshold = 0.3  # seconds\n",
    "isi = np.diff(spike_times)\n",
    "\n",
    "bursts = []\n",
    "current_burst = [spike_times[0]]\n",
    "for i in range(1, len(isi)):\n",
    "    if isi[i-1] < burst_threshold:\n",
    "        current_burst.append(spike_times[i])\n",
    "    else:\n",
    "        if len(current_burst) > 1:\n",
    "            bursts.append((current_burst[0], current_burst[-1]))\n",
    "        current_burst = [spike_times[i]]\n",
    "if len(current_burst) > 1:\n",
    "    bursts.append((current_burst[0], current_burst[-1]))\n",
    "\n",
    "print(f\"Total bursts detected: {len(bursts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burst Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_wave_bursts, parabolic_bursts, other_bursts = [], [], []\n",
    "\n",
    "for i, (start, end) in enumerate(bursts):\n",
    "    burst_mask = (time >= start) & (time <= end)\n",
    "    burst_min = np.min(signal[burst_mask])\n",
    "    \n",
    "    prev_mean = np.mean(signal[(time > bursts[i-1][1]) & (time < start)]) if i>0 else np.nan\n",
    "    next_mean = np.mean(signal[(time > end) & (time < bursts[i+1][0])]) if i < len(bursts)-1 else np.nan\n",
    "    inter_mean = np.nanmean([prev_mean, next_mean])\n",
    "    \n",
    "    if burst_min > inter_mean:\n",
    "        square_wave_bursts.append((start, end))\n",
    "    elif burst_min < inter_mean:\n",
    "        parabolic_bursts.append((start, end))\n",
    "    else:\n",
    "        other_bursts.append((start, end))\n",
    "\n",
    "print(f\"Square Wave: {len(square_wave_bursts)}, Parabolic: {len(parabolic_bursts)}, Other: {len(other_bursts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to normalize bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y(segment):\n",
    "    return (segment - np.mean(segment)) / np.std(segment)\n",
    "\n",
    "def rescale_x(t_segment, segment, n_points=100):\n",
    "    f = interp1d(np.linspace(0,1,len(segment)), segment)\n",
    "    return f(np.linspace(0,1,n_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract normalized bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_normalized_bursts(burst_list, signal, time, n_points=100):\n",
    "    normalized_bursts = []\n",
    "    for start, end in burst_list:\n",
    "        mask = (time >= start) & (time <= end)\n",
    "        segment = signal[mask]\n",
    "        segment_rescaled = rescale_x(time[mask], segment, n_points)\n",
    "        segment_normalized = normalize_y(segment_rescaled)\n",
    "        normalized_bursts.append(segment_normalized)\n",
    "    return normalized_bursts\n",
    "\n",
    "square_bursts_norm = extract_normalized_bursts(square_wave_bursts, signal, time)\n",
    "parabolic_bursts_norm = extract_normalized_bursts(parabolic_bursts, signal, time)\n",
    "other_bursts_norm = extract_normalized_bursts(other_bursts, signal, time)\n",
    "\n",
    "all_normalized_bursts = square_bursts_norm + parabolic_bursts_norm + other_bursts_norm\n",
    "labels = ([0]*len(square_bursts_norm) + [1]*len(parabolic_bursts_norm) + [2]*len(other_bursts_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapelet Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.shapelets import LearningShapelets\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare data\n",
    "X = np.array(all_normalized_bursts)[:, :, np.newaxis]\n",
    "y = np.array(labels)\n",
    "X = TimeSeriesScalerMinMax().fit_transform(X)\n",
    "\n",
    "# Learning shapelets\n",
    "n_shapelets_per_size = {X.shape[1]: 2}\n",
    "shp_clf = LearningShapelets(\n",
    "    n_shapelets_per_size=n_shapelets_per_size,\n",
    "    weight_regularizer=0.0001,\n",
    "    optimizer=Adam(0.01),\n",
    "    max_iter=300,\n",
    "    verbose=0,\n",
    "    scale=False,\n",
    "    random_state=42\n",
    ")\n",
    "shp_clf.fit(X, y)\n",
    "\n",
    "# Transform distances to 2D\n",
    "distances = shp_clf.transform(X).reshape((-1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Shapelets and Distance Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viridis = cm.get_cmap('viridis', 4)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10,6))\n",
    "plt.scatter(distances[:,0], distances[:,1], c=[viridis(l/2) for l in y], edgecolors='k')\n",
    "plt.xlabel('Distance to Shapelet 1')\n",
    "plt.ylabel('Distance to Shapelet 2')\n",
