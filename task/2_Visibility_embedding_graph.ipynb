{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Spike and Burst Analysis with Visibility Graphs\n",
    "\n",
    "This notebook provides a detailed workflow for analyzing electrophysiological ABF recordings.\n",
    "It goes through the following steps:\n",
    "1. Loading ABF files and concatenating sweeps\n",
    "2. Spike detection\n",
    "3. Burst detection\n",
    "4. Burst classification\n",
    "5. Saving bursts to CSV\n",
    "6. Visibility graph creation for each burst\n",
    "7. 2D and 3D embeddings of the burst graphs\n",
    "8. Visualization of spikes, bursts, and graphs\n",
    "9. Saving nodes and edges data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyabf  # Reads ABF electrophysiology files\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # DataFrames and CSV handling\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D plotting support\n",
    "import networkx as nx  # Graph creation and analysis\n",
    "from scipy.signal import find_peaks  # Spike detection\n",
    "from sklearn.decomposition import TruncatedSVD  # Embedding\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load ABF File and Concatenate Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"bursting/cell89basal.abf\"\n",
    "abf = pyabf.ABF(file_path)\n",
    "\n",
    "# Concatenate sweeps properly\n",
    "sweeps = []\n",
    "for i in range(abf.sweepCount):\n",
    "    abf.setSweep(i)\n",
    "    sweeps.append(np.copy(abf.sweepY))\n",
    "signal = np.concatenate(sweeps) if len(sweeps) > 0 else np.array([])\n",
    "\n",
    "dt = 1.0 / abf.dataRate if abf.dataRate > 0 else 1.0\n",
    "time = np.arange(len(signal)) * dt\n",
    "\n",
    "if len(time) > 0:\n",
    "    print(f\"File: {file_path} | sweeps: {abf.sweepCount} | total duration: {time[-1]:.2f} s\")\n",
    "else:\n",
    "    print(f\"File: {file_path} loaded but empty signal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Spike Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -35  # mV\n",
    "if len(signal) > 0:\n",
    "    spike_indices, props = find_peaks(signal, height=threshold)\n",
    "    spike_times = time[spike_indices]\n",
    "else:\n",
    "    spike_indices = np.array([])\n",
    "    spike_times = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Burst Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts = []\n",
    "if len(spike_times) > 0:\n",
    "    isi = np.diff(spike_times)\n",
    "    burst_threshold = 0.3  # seconds\n",
    "    current_burst = [0]\n",
    "\n",
    "    for i in range(1, len(spike_times)):\n",
    "        if isi[i-1] < burst_threshold:\n",
    "            current_burst.append(i)\n",
    "        else:\n",
    "            if len(current_burst) > 1:\n",
    "                bursts.append(current_burst)\n",
    "            current_burst = [i]\n",
    "    if len(current_burst) > 1:\n",
    "        bursts.append(current_burst)\n",
    "\n",
    "    print(f\"Detected {len(bursts)} bursts\")\n",
    "\n",
    "    isi_per_spike_burst = np.zeros(len(spike_times))\n",
    "    for burst in bursts:\n",
    "        for i, idx in enumerate(burst):\n",
    "            isi_per_spike_burst[idx] = 0 if i == 0 else (spike_times[idx] - spike_times[burst[i - 1]]) * 1000\n",
    "else:\n",
    "    isi_per_spike_burst = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Burst Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_wave_bursts, parabolic_bursts, other_bursts = [], [], []\n",
    "burst_types = {}\n",
    "baseline_window = 0.05  # seconds\n",
    "\n",
    "for i, burst in enumerate(bursts):\n",
    "    t_start = spike_times[burst[0]]\n",
    "    t_end = spike_times[burst[-1]]\n",
    "\n",
    "    burst_mask = (time >= t_start) & (time <= t_end)\n",
    "    burst_min = np.min(signal[burst_mask]) if np.any(burst_mask) else np.nan\n",
    "\n",
    "    prev_mask = (time >= max(0, t_start - baseline_window)) & (time < t_start)\n",
    "    next_mask = (time > t_end) & (time <= min(time[-1], t_end + baseline_window))\n",
    "\n",
    "    prev_mean = np.mean(signal[prev_mask]) if np.any(prev_mask) else np.nan\n",
    "    next_mean = np.mean(signal[next_mask]) if np.any(next_mask) else np.nan\n",
    "    inter_mean = np.nanmean([prev_mean, next_mean]) if not np.isnan(prev_mean) or not np.isnan(next_mean) else np.nanmean(signal)\n",
    "\n",
    "    if np.isnan(inter_mean) or np.isnan(burst_min):\n",
    "        burst_types[tuple(burst)] = \"Other\"\n",
    "        other_bursts.append(burst)\n",
    "    elif burst_min > inter_mean:\n",
    "        burst_types[tuple(burst)] = \"Square Wave\"\n",
    "        square_wave_bursts.append(burst)\n",
    "    elif burst_min < inter_mean:\n",
    "        burst_types[tuple(burst)] = \"Parabolic\"\n",
    "        parabolic_bursts.append(burst)\n",
    "    else:\n",
    "        burst_types[tuple(burst)] = \"Other\"\n",
    "        other_bursts.append(burst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Bursts to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burst_list = []\n",
    "for idx, burst in enumerate(bursts):\n",
    "    burst_type = burst_types.get(tuple(burst), \"Other\")\n",
    "    burst_list.append([idx + 1, spike_times[burst[0]], spike_times[burst[-1]], burst_type])\n",
    "\n",
    "df_bursts_all = pd.DataFrame(burst_list, columns=[\"Burst_Number\", \"Start_Time_s\", \"End_Time_s\", \"Type\"])\n",
    "df_bursts_all.to_csv(\"burst_basic_info_cell89_all_bursts.csv\", index=False)\n",
    "df_bursts_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visibility Graphs and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_map = {\"Square Wave\": \"blue\", \"Parabolic\": \"green\", \"Other\": \"orange\"}\n",
    "nodes_list, edges_list = [], []\n",
    "\n",
    "for b_idx, burst in enumerate(bursts):\n",
    "    burst_type = burst_types.get(tuple(burst), \"Other\")\n",
    "    n_nodes = len(burst)\n",
    "    if n_nodes == 0:\n",
    "        continue\n",
    "\n",
    "    x_peaks = np.arange(n_nodes)\n",
    "    y_peaks = isi_per_spike_burst[burst]\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n_nodes))\n",
    "\n",
    "    for a in range(n_nodes):\n",
    "        for b in range(a + 1, n_nodes):\n",
    "            visible = True\n",
    "            for c in range(a + 1, b):\n",
    "                y_line = y_peaks[a] + (y_peaks[b] - y_peaks[a]) * ((x_peaks[c] - x_peaks[a]) / (x_peaks[b] - x_peaks[a]))\n",
    "                if y_peaks[c] >= y_line:\n",
    "                    visible = False\n",
    "                    break\n",
    "            if visible:\n",
    "                G.add_edge(a, b)\n",
    "                edges_list.append([b_idx + 1, burst_type, a, b])\n",
    "\n",
    "    A = nx.to_numpy_array(G)\n",
    "    if A.shape[0] >= 2:\n",
    "        embedding_2d = TruncatedSVD(n_components=2, random_state=42).fit_transform(A)\n",
    "    else:\n",
    "        embedding_2d = np.zeros((n_nodes, 2))\n",
    "\n",
    "    if A.shape[0] >= 3:\n",
    "        embedding_3d = TruncatedSVD(n_components=3, random_state=42).fit_transform(A)\n",
    "    else:\n",
    "        embedding_3d = np.zeros((n_nodes, 3))\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        nodes_list.append([b_idx + 1, burst_type, i,\n",
    "                           embedding_2d[i, 0], embedding_2d[i, 1],\n",
    "                           embedding_3d[i, 0], embedding_3d[i, 1], embedding_3d[i, 2],\n",
    "                           burst[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Nodes and Edges CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = pd.DataFrame(nodes_list, columns=[\"Burst_Number\", \"Type\", \"Node_ID\",\n",
    "                                             \"X_2D\", \"Y_2D\", \"X_3D\", \"Y_3D\", \"Z_3D\",\n",
    "                                             \"Spike_Global_Index\"])\n",
    "df_edges = pd.DataFrame(edges_list, columns=[\"Burst_Number\", \"Type\", \"Node1_ID\", \"Node2_ID\"])\n",
    "\n",
    "df_nodes.to_csv(\"burst_nodes_all.csv\", index=False)\n",
    "df_edges.to_csv(\"burst_edges_all.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
